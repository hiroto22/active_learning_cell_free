{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd2f8c-d423-493b-aa14-358c108d4e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af1b43e0-677e-4ba7-8a87-42e52301fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn import decomposition\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# This is data that has already been geenrated due to DOE methodology\n",
    "import warnings\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5448f21f-2c58-43c7-bb76-442bdeb6eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_for_data = \"data/no_controls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466525b-3450-4de0-a31b-dd1921d69a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "728d311b-745d-438e-9509-0d130cb9ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data from the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d2a8fcd-0850-4576-b7b8-acf26dbe57f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_1 = \"{}/plate_AL_1_raw_yield_and_std.csv\".format(folder_for_data)\n",
    "plate_1_array = np.genfromtxt(plate_1, delimiter=',', skip_header = 1,dtype = \"float\")\n",
    "\n",
    "plate_2 = \"{}/plate_AL_2_raw_yield_and_std.csv\".format(folder_for_data)\n",
    "plate_2_array = np.genfromtxt(plate_2, delimiter=',', skip_header = 1,dtype = \"float\")\n",
    "\n",
    "plate_3 = \"{}/plate_AL_3_raw_yield_and_std.csv\".format(folder_for_data)\n",
    "plate_3_array = np.genfromtxt(plate_3, delimiter=',', skip_header = 1,dtype = \"float\")\n",
    "\n",
    "plate_4 = \"{}/plate_AL_4_raw_yield_and_std.csv\".format(folder_for_data)\n",
    "plate_4_array = np.genfromtxt(plate_4, delimiter=',', skip_header = 1,dtype = \"float\")\n",
    "\n",
    "plate_5 = \"{}/plate_AL_5_raw_yield_and_std.csv\".format(folder_for_data)\n",
    "plate_5_array = np.genfromtxt(plate_5, delimiter=',', skip_header = 1,dtype = \"float\")\n",
    "\n",
    "plate_6 = \"{}/plate_AL_6_raw_yield_and_std.csv\".format(folder_for_data)\n",
    "plate_6_array = np.genfromtxt(plate_6, delimiter=',', skip_header = 1,dtype = \"float\")\n",
    "\n",
    "plate_7 = \"{}/plate_AL_7_raw_yield_and_std.csv\".format(folder_for_data)\n",
    "plate_7_array = np.genfromtxt(plate_7, delimiter=',', skip_header = 1,dtype = \"float\")\n",
    "\n",
    "plate_8 = \"{}/plate_AL_8_raw_yield_and_std.csv\".format(folder_for_data)\n",
    "plate_8_array = np.genfromtxt(plate_8, delimiter=',', skip_header = 1,dtype = \"float\")\n",
    "\n",
    "plate_9 = \"{}/plate_AL_9_raw_yield_and_std.csv\".format(folder_for_data)\n",
    "plate_9_array = np.genfromtxt(plate_9, delimiter=',', skip_header = 1,dtype = \"float\")\n",
    "\n",
    "plate_10 = \"{}/plate_AL_10_raw_yield_and_std.csv\".format(folder_for_data)\n",
    "plate_10_array = np.genfromtxt(plate_10, delimiter=',', skip_header = 1,dtype = \"float\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7c364-bb1f-47c4-b897-8ec537d4f9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b584cd03-e112-4832-8bd7-70ca14bd79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing plates to test\n",
    "#This study was performed using a random seed, \n",
    "#so the methodology presented below holds but exact values used for the next iteration should not be expected.\n",
    "#Exemple on filling of data is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb8221bf-9beb-4342-a338-1c21bc5afb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_iteration(iteration_number = 1):\n",
    "    \"\"\"\n",
    "    Obtain data from the desired number of iterations, and not the whole data.\n",
    "    [[.....],[.....]]\n",
    "    \"\"\"\n",
    "    full_plates_list = [plate_1_array, plate_2_array, plate_3_array, plate_4_array, plate_5_array,\n",
    "                        plate_6_array, plate_7_array, plate_8_array, plate_9_array, plate_10_array]\n",
    "    selected_plates = full_plates_list[0:iteration_number]\n",
    "    current_data = np.concatenate(selected_plates, axis = 0)\n",
    "    return(current_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c972c0-19d2-4b3c-82f7-32db2ff27147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc98f02b-6205-43d7-8897-b47dd2c105df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data_from_iteration(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d61a43b4-b2a3-4d90-93e6-f5450782a0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23468376-3106-474d-8d13-9723ea561251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.6500e-01 2.0400e-02 7.8000e-02 ... 4.0000e+01 0.0000e+00 1.3100e-02]\n",
      " [3.3000e-01 6.8000e-02 2.6000e-01 ... 8.0000e+01 0.0000e+00 9.5000e-03]\n",
      " [3.3000e-01 6.8000e-02 2.6000e-01 ... 8.0000e+01 0.0000e+00 7.7000e-02]\n",
      " ...\n",
      " [3.3000e-02 2.0400e-02 2.6000e-02 ... 4.0000e+01 1.4587e+00 3.0100e-02]\n",
      " [3.3000e-02 6.8000e-03 7.8000e-02 ... 4.0000e+01 1.0536e+00 3.5100e-02]\n",
      " [1.6500e-01 2.0400e-02 2.6000e-02 ... 4.0000e+01 1.2650e+00 3.3400e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb75d6e1-6955-4224-b952-2de04906e600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.65e-01 2.04e-02 7.80e-02 ... 1.20e+00 3.75e-01 4.00e+01]\n",
      " [3.30e-01 6.80e-02 2.60e-01 ... 4.00e-01 7.50e-01 8.00e+01]\n",
      " [3.30e-01 6.80e-02 2.60e-01 ... 4.00e+00 7.50e-01 8.00e+01]\n",
      " ...\n",
      " [3.30e-02 2.04e-02 2.60e-02 ... 2.00e+00 7.50e-02 4.00e+01]\n",
      " [3.30e-02 6.80e-03 7.80e-02 ... 2.00e+00 7.50e-02 4.00e+01]\n",
      " [1.65e-01 2.04e-02 2.60e-02 ... 2.00e+00 7.50e-02 4.00e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(data[:, 0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63eba74-b63f-491c-97b9-2be7ed6639f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9639288-31ad-42d7-9c7a-aeaf761a9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining tested concentrations and volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95f761fb-36ab-4c5c-a563-6fba40cedca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum concentrations for each component\n",
    "\n",
    "extract_max = 30\n",
    "mg_gluta_max = 4\n",
    "K_gluta_max = 80\n",
    "aa_max = 1.5\n",
    "peg_max = 2\n",
    "hepes_max = 50\n",
    "trna_max = 0.2\n",
    "coa_max = 0.26\n",
    "nad_max = 0.33\n",
    "camp_max = 0.75\n",
    "folinic_acid_max = 0.068\n",
    "spemidine_max = 1\n",
    "pga_max = 30\n",
    "nucleo_mix_max = 1.5\n",
    "DNA_max = 50\n",
    "\n",
    "promoter_max = 10\n",
    "RBS_max = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb3f4075-4e0c-48c1-b039-58c8eab5687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concentration data: tested concentrations for each reagent\n",
    "\n",
    "mg_gluta_conc = [0.1, 0.3, 0.5, 1]\n",
    "K_gluta_conc = [0.1, 0.3, 0.5, 1]\n",
    "aa_conc = [0.1, 0.3, 0.5, 1]\n",
    "trna_conc = [0.1, 0.3, 0.5, 1]\n",
    "coa_conc = [0.1, 0.3, 0.5, 1]\n",
    "nad_conc = [0.1, 0.3, 0.5, 1]\n",
    "camp_conc = [0.1, 0.3, 0.5, 1]\n",
    "folinic_acid_conc = [0.1, 0.3, 0.5, 1]\n",
    "spermidine_conc = [0.1, 0.3, 0.5, 1]\n",
    "pga_conc = [0.1, 0.3, 0.5, 1]\n",
    "nucleo_conc = [0.1, 0.3, 0.5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e3e64-2556-4944-88a8-e2066d7e0901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f01e47ef-fbd2-404e-a53b-60dfcd0e07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping data for following tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54c2c81a-ffe8-4e32-94bd-8e051c55e633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33, 0.068, 0.26, 1.5, 1.0, 30.0, 1.5, 0.2, 4.0, 0.75, 80.0]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "各列のデータの最大値をinitial_maxに入れて最大値の標準化に使う\n",
    "X_dataには0〜11列のdataが入る\n",
    "y_dataには収量の列が入る\n",
    "'''\n",
    "\n",
    "\n",
    "initial_max = []  # This stores maximum values for normalisation later on.\n",
    "X_data, y_data, y_std_data = data[:, 0:11], data[:, 11], data[:, 12]\n",
    "for i in range(X_data.shape[1]):\n",
    "    initial_max.append(copy.deepcopy(max(X_data[:,i])))\n",
    "    X_data[:,i] = X_data[:,i]/max(X_data[:,i])\n",
    "print(initial_max)\n",
    "# my_current_data = np.concatenate((plate_1a_array, plate_1b_array), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e560ee-3999-431d-a14c-72e2a2d5905d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdfaa164-1c78-49f8-b95f-a9e198ccac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5358\n"
     ]
    }
   ],
   "source": [
    "# Maximum yield attained at those iterations\n",
    "best_of = np.amax(y_data)\n",
    "print(best_of)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5058c98e-352c-4fd5-a3f7-338936fa9fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5    0.3    0.3    ... 0.5    0.     0.0131]\n",
      " [1.     1.     1.     ... 1.     0.     0.0095]\n",
      " [1.     1.     1.     ... 1.     0.     0.077 ]\n",
      " ...\n",
      " [0.1    0.3    0.1    ... 0.5    1.4587 0.0301]\n",
      " [0.1    0.1    0.3    ... 0.5    1.0536 0.0351]\n",
      " [0.5    0.3    0.1    ... 0.5    1.265  0.0334]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0bf1f105-e877-45fc-95e1-dc75617b1522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795ebba-7d61-4f73-aa28-eb219174e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261cc14b-85f2-4ec4-896a-45a19520850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_in_array_index(new_sample, array, size = 11):\n",
    "    \"\"\"\n",
    "    Verify if a sample is present in an array.\n",
    "    \"\"\"\n",
    "    present = False\n",
    "    new_sample = np.reshape(np.array(new_sample), (1,size))\n",
    "    for i in range(array.shape[0]):\n",
    "        if np.array_equiv(array[i,:],new_sample):\n",
    "            present = True\n",
    "            break\n",
    "    return(present, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfffec-39e3-4d21-82b6-57f969dad7b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (289662003.py, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_78/289662003.py\"\u001b[0;36m, line \u001b[0;32m68\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "X　濃度データ\n",
    "y　収量データ\n",
    "'''\n",
    "\n",
    "def select_current_best_model(X, y, models_number = 10, \n",
    "                              verbose = False, \n",
    "                             MLP = True,\n",
    "                             visu = False,\n",
    "                             model_name = \"test\"):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    trained_model_list = []\n",
    "    # Training all models　\n",
    "    #それぞれ hidden_layer_sizeを変えて学習させている\n",
    "    for i in range(models_number):\n",
    "        X_train, y_train = X, y\n",
    "        if MLP:\n",
    "            other_MLP = MLPRegressor(hidden_layer_sizes  = (10, 100,100, 20), solver =\"adam\", max_iter=20000, \n",
    "                                      early_stopping = True, learning_rate = \"adaptive\")\n",
    "            other_MLP.fit(X_train, y_train.flatten())    \n",
    "            trained_model_list.append(other_MLP)\n",
    "\n",
    "            big_MLP = MLPRegressor(hidden_layer_sizes  = (100,100, 20),solver =\"adam\", max_iter=20000, \n",
    "                                      early_stopping = True, learning_rate = \"adaptive\")\n",
    "            big_MLP.fit(X_train, y_train.flatten())    \n",
    "            trained_model_list.append(big_MLP)\n",
    "\n",
    "\n",
    "            medium_MLP = MLPRegressor(hidden_layer_sizes  = (40, 10), solver =\"adam\", max_iter=20000, \n",
    "                                      early_stopping = True, learning_rate = \"adaptive\")\n",
    "            medium_MLP.fit(X_train, y_train.flatten())    \n",
    "            trained_model_list.append(medium_MLP)\n",
    "  \n",
    "            small_MLP = MLPRegressor(hidden_layer_sizes  = (10), solver =\"adam\", max_iter=20000, \n",
    "                                      early_stopping = True, learning_rate = \"adaptive\")\n",
    "            small_MLP.fit(X_train, y_train.flatten())    \n",
    "            trained_model_list.append(small_MLP)\n",
    "            \n",
    "        \n",
    "    # Evaluating all \n",
    "    all_scores = []\n",
    "    for i in range(len(trained_model_list)):\n",
    "        selected_mdoel = trained_model_list[i]\n",
    "        y_pred = selected_mdoel.predict(X)\n",
    "        score = sklearn.metrics.r2_score(y, y_pred)\n",
    "        all_scores.append(score)\n",
    "\n",
    "    try:\n",
    "        best_index = all_scores.index(max(all_scores))\n",
    "        best_score = all_scores[best_index]\n",
    "    except ValueError:\n",
    "        best_index = 0\n",
    "        \n",
    "    if verbose:\n",
    "        print(all_scores)\n",
    "        print(\"Best index is {}\".format(best_index))\n",
    "        print(\"Best score is {}\".format(best_score))\n",
    "        \n",
    "    best_model = trained_model_list[best_index]\n",
    "    \n",
    "    if visu:        \n",
    "        model = best_model\n",
    "        y_pred = model.predict(X)\n",
    "        score = sklearn.metrics.r2_score(y, y_pred)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(y, y_pred, edgecolors=(0, 0, 0))\n",
    "        ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "        ax.set_xlabel('Measured')\n",
    "        ax.set_title(\"Model prediction for model {}: {}\".format(model_name, score))\n",
    "        ax.set_ylabel('Predicted')\n",
    "        plt.show()\n",
    "    return(best_model, best_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e29a4-4b15-49b5-aacd-bb80c27e6b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e0973-ea5c-4205-9648-421ac2498bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This part of the code trains the model on all available data, to generate an ensemble of models used for later predictions. \n",
    "Parameters used for the study were:\n",
    "\n",
    "n = 25 models for ensemble size\n",
    "models_number = 10 for repeating training\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b3445-e19d-4f50-bbac-028e7ab4d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mlp_start_time = time.time()\n",
    "ensemble_MLP = []\n",
    "n = 3\n",
    "# n = 25\n",
    "for i in range(n):\n",
    "    current_best, best_score = select_current_best_model(X_data, y_data, \n",
    "                                             models_number = 3, verbose = False,\n",
    "                                             MLP = True,\n",
    "                                             visu = True,\n",
    "                                             model_name = i)\n",
    "    ensemble_MLP.append(current_best)\n",
    "training_mlp_end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf6151-6899-4cd5-8fbd-029cf3ffaff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d864272-9f80-430f-81d7-8ddbddc4de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the models do\n",
    "for i in range(len(ensemble_MLP)):\n",
    "    model = ensemble_MLP[i]\n",
    "    y_pred = model.predict(X_data)\n",
    "    score = sklearn.metrics.r2_score(y_data, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y_data, y_pred, edgecolors=(0, 0, 0))\n",
    "    ax.errorbar(y_data, y_pred,xerr = y_std_data, ls='none')\n",
    "    ax.plot([y_data.min(), y_data.max()], [y_data.min(), y_data.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_title(\"Model prediction for model {}: {}\".format(i, score))\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb19d08-b3e5-4214-a7cb-b2e7bc420ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルが何をしているのかを可視化している\n",
    "all_predictions = None\n",
    "for model in ensemble_MLP:\n",
    "    y_pred = model.predict(X_data)\n",
    "    answer_array_pred = y_pred.reshape(X_data.shape[0], -1)\n",
    "    if all_predictions is None:\n",
    "        all_predictions = y_pred.reshape(X_data.shape[0], -1)\n",
    "    else:\n",
    "        all_predictions =np.concatenate((all_predictions, y_pred.reshape(X_data.shape[0], -1)), axis = 1)\n",
    "\n",
    "y_pred, y_pred_std = np.mean(all_predictions, axis = 1), np.std(all_predictions, axis = 1)\n",
    "score = sklearn.metrics.r2_score(y_data, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_data, y_pred, edgecolors=(0, 0, 0))\n",
    "ax.errorbar(y_data, y_pred, xerr = y_std_data, yerr = y_pred_std, ls='none')\n",
    "ax.plot([y_data.min(), y_data.max()], [y_data.min(), y_data.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_title(\"Model prediction for ensemble of models: {}\".format(score))\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4275438d-a25b-4d24-8ec8-5348d4860b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6bf2f-8006-4e8f-b7b7-cadafb11a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions for suggesting experimental points\n",
    "The next step once our ensemble is trained is to use it for the active learning itself.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787d360-d8d4-43d2-a5b4-dcc672c0cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_grid(array_to_avoid, sample_size = 100, normalisation = True, verbose = True):\n",
    "    \"\"\"\n",
    "    Generates a random grid of desired size avoiding predefined concentrations.\n",
    "    Can be quite long for big arrays, as it verifies combinations were not previously sampled.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Started generating the random array\")\n",
    "    active_learning_array = None\n",
    "    answerSize = 0\n",
    "    while answerSize < sample_size:\n",
    "        \n",
    "        this_sample = np.random.randint(0, high=4, size=11, dtype='int')  # 11 concentrations that can move\n",
    "        if normalisation:\n",
    "            this_sample_conc = [nad_conc[this_sample[0]], \n",
    "                               folinic_acid_conc[this_sample[1]],\n",
    "                                coa_conc[this_sample[2]], \n",
    "                                nucleo_conc[this_sample[3]],\n",
    "                                spermidine_conc[this_sample[4]],\n",
    "                                pga_conc[1], \n",
    "                                aa_conc[this_sample[6]], \n",
    "                                trna_conc[this_sample[7]],\n",
    "                                mg_gluta_conc[this_sample[8]], \n",
    "                                camp_conc[this_sample[9]], \n",
    "                                K_gluta_conc[this_sample[10]]                          \n",
    "                                ]\n",
    "        else:\n",
    "            this_sample_conc = [nad_conc[this_sample[0]] * nad_max, \n",
    "                               folinic_acid_conc[this_sample[1]] * folinic_acid_max,\n",
    "                                coa_conc[this_sample[2]] * coa_max, \n",
    "                                nucleo_conc[this_sample[3]] * nucleo_mix_max,\n",
    "                                spermidine_conc[this_sample[4]] * spemidine_max,\n",
    "                                pga_conc[1] * pga_max, \n",
    "                                aa_conc[this_sample[6]] * aa_max, \n",
    "                                trna_conc[this_sample[7]] * trna_max,\n",
    "                                mg_gluta_conc[this_sample[8]] * mg_gluta_max, \n",
    "                                camp_conc[this_sample[9]] * camp_max, \n",
    "                                K_gluta_conc[this_sample[10]] * K_gluta_max                          \n",
    "                                ]\n",
    "        this_sample_conc = np.reshape(this_sample_conc, (1, 11))\n",
    "        if not present_in_array_index(this_sample_conc, array_to_avoid, size = 11)[0]:\n",
    "            answerSize = answerSize + 1\n",
    "            if active_learning_array is None:\n",
    "                active_learning_array = this_sample_conc\n",
    "            else:\n",
    "                active_learning_array = np.concatenate((active_learning_array, this_sample_conc), axis = 0)\n",
    "            array_to_avoid = np.concatenate((array_to_avoid, this_sample_conc), axis = 0)\n",
    "    if verbose:\n",
    "         print(\"Finished generating the random array\")\n",
    "    return(active_learning_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af17b5-5ad1-4a19-8bb3-b7c30246384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_predictions_from_ensemble_model(ensemble_of_models, array_to_avoid, \n",
    "                                                total_sampling_size = 1000,\n",
    "                                                sample_size = 500,\n",
    "                                                exploitation = 1,\n",
    "                                                exploration = 1.41, \n",
    "                                                normalisation = True, \n",
    "                                               initial_max = initial_max,\n",
    "                                               verbose = True): \n",
    "    \"\"\"\n",
    "    Heart of the active learning process.\n",
    "    Uses a pre-trained ensemble of models to predict on randomly chosen combinations and selects next experiments.\n",
    "    total_sampling_size: number of combinations to randomly sample\n",
    "    sample_size: number to export for further analysis\n",
    "    exploitation: weighting of the yield for UCT equivalent\n",
    "    exploration: weighting of the ucnertainty (yield std) for UCT equivalent \n",
    "    normalisation: normalise concentrations (maximum at 1) \n",
    "    \"\"\"\n",
    "    # Random concentrations to test\n",
    "    active_learning_array  = generate_random_grid(array_to_avoid, sample_size = total_sampling_size, normalisation = normalisation)\n",
    "    # Predicting the full random grid\n",
    "    answer_array_pred = np.empty\n",
    "    all_predictions = None\n",
    "    if verbose:\n",
    "        print(\"Starting ensemble predictions\")\n",
    "    for model in ensemble_of_models:\n",
    "        y_pred = model.predict(active_learning_array)\n",
    "        answer_array_pred = y_pred.reshape(active_learning_array.shape[0], -1)\n",
    "        if all_predictions is None:\n",
    "            all_predictions = y_pred.reshape(active_learning_array.shape[0], -1)\n",
    "        else:\n",
    "            all_predictions =np.concatenate((all_predictions, y_pred.reshape(active_learning_array.shape[0], -1)), axis = 1)\n",
    "    if verbose:\n",
    "        print(\"Finished ensemble predictions\")    \n",
    "    # Otaining mean and std for predicted array\n",
    "    y_pred, y_pred_std = np.mean(all_predictions, axis = 1), np.std(all_predictions, axis = 1)\n",
    "    # Create the array to maximise, balancing between exploration and exploitation\n",
    "    array_to_maximise = copy.deepcopy(exploitation * y_pred + exploration * y_pred_std)\n",
    "    # Select arrays depending on choice of way to eplore: only uncertainty, only yield, or a mix of both.\n",
    "    conditions_list_pure_exploitation = []\n",
    "    for count in range(sample_size):\n",
    "        i = np.argmax(y_pred)\n",
    "        conditions_list_pure_exploitation.append(int(i))\n",
    "        if verbose:\n",
    "            print(\"Maximising sample {} is yield: {}, std = {}\".format(i, y_pred[i], y_pred_std[i]))\n",
    "        y_pred[i] = -1\n",
    "        \n",
    "    conditions_list_pure_exploration = []\n",
    "    for count in range(sample_size):\n",
    "        i = np.argmax(y_pred_std)\n",
    "        conditions_list_pure_exploration.append(int(i))\n",
    "        if verbose:\n",
    "            print(\"Maximising sample {} is yield: {}, std = {}\".format(i, y_pred[i], y_pred_std[i]))\n",
    "        y_pred_std[i] = -1\n",
    "    \n",
    "    conditions_list = []\n",
    "    for count in range(sample_size):\n",
    "        i = np.argmax(array_to_maximise)\n",
    "        conditions_list.append(int(i))\n",
    "        if verbose:\n",
    "            print(\"Maximising sample {} is yield: {}, std = {}\".format(i, y_pred[i], y_pred_std[i]))\n",
    "        array_to_maximise[i] = -1\n",
    "\n",
    "    if normalisation:\n",
    "        for i in range(active_learning_array.shape[1]):\n",
    "            active_learning_array[:,i] = active_learning_array[:,i]*initial_max[i]\n",
    "    else:\n",
    "        active_learning_array =  active_learning_array\n",
    "            \n",
    "    conditions_to_test = active_learning_array[conditions_list,:]\n",
    "    conditions_to_test_eploration = active_learning_array[conditions_list_pure_exploration,:]\n",
    "    conditions_to_test_exploitation = active_learning_array[conditions_list_pure_exploitation,:]\n",
    "    return(conditions_to_test, conditions_to_test_eploration, conditions_to_test_exploitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376996e3-dd02-45aa-9154-6939019db027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_array_of_elements_of_interest(array, file_place):\n",
    "    \"\"\"\n",
    "    Export an array as a csv in the destined place\n",
    "    \"\"\"\n",
    "    fieldnames = [\"nad\", \"folinic_acid\", \"DNA\", \"coa\", \"RBS\", \"peg\", \"nucleo_mix\", \n",
    "                  \"spermidin\", \"pga\", \"aa\", \"trna\", \"mg_gluta\", \"hepes\", \"camp\", \"K_gluta\", \"promoter\"]\n",
    "    export_as_list = []\n",
    "    # They were not modified in this project.\n",
    "    peg = 1\n",
    "    hepes = 1\n",
    "    \n",
    "    for row in array:\n",
    "        new_dict = {}\n",
    "        new_dict[\"nad\"] = round(float(row[0]), 5)\n",
    "        new_dict[\"folinic_acid\"] = round(float(row[1]), 5)\n",
    "        new_dict[\"DNA\"] = 50\n",
    "        new_dict[\"coa\"] = round(float(row[2]), 5)\n",
    "        new_dict[\"RBS\"] = 10\n",
    "        new_dict[\"peg\"] = 2\n",
    "        new_dict[\"nucleo_mix\"] = round(float(row[3]), 5)\n",
    "        new_dict[\"spermidin\"] = round(float(row[4]), 5)\n",
    "        new_dict[\"pga\"] = round(float(row[5]), 5)\n",
    "        new_dict[\"aa\"] = round(float(row[6]), 5)\n",
    "        new_dict[\"trna\"] = round(float(row[7]), 5)\n",
    "        new_dict[\"mg_gluta\"] = round(float(row[8]), 4)\n",
    "        new_dict[\"hepes\"] = 50\n",
    "        new_dict[\"camp\"] = round(float(row[9]), 4)\n",
    "        new_dict[\"K_gluta\"] = round(float(row[10]), 4)\n",
    "        new_dict[\"promoter\"] = 10\n",
    "        export_as_list.append(new_dict)\n",
    "    with open(file_place, \"w\") as csv_handle:\n",
    "        csv_writer = csv.DictWriter(csv_handle, fieldnames, restval='', extrasaction='ignore')\n",
    "        csv_writer.writeheader()\n",
    "        for result in export_as_list:\n",
    "            csv_writer.writerow(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d12a344-3670-41a4-aa1a-246c34dff9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5748dd-4fdd-45ce-bb27-706eca8f4b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Active learning\n",
    "#In the article, we used a total sampling size of 100000, and exported samples of 500 combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e0fda-efd6-4d95-8e76-ce6540cac336",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_to_test, conditions_to_test_eploration, conditions_to_test_exploitation = select_best_predictions_from_ensemble_model(ensemble_of_models = ensemble_MLP, \n",
    "                                            array_to_avoid = X_data, \n",
    "                                            total_sampling_size = 1000, \n",
    "                                            verbose = False,\n",
    "                                            sample_size = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cdb51a-2d35-4858-b21c-86fff8064d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = \"showcase_AL\"\n",
    "export_array_of_elements_of_interest(conditions_to_test, file_place = \"{}.csv\".format(base_name))\n",
    "export_array_of_elements_of_interest(conditions_to_test_eploration, file_place = \"{}_exploration.csv\".format(base_name))\n",
    "export_array_of_elements_of_interest(conditions_to_test_exploitation, file_place = \"{}_exploitation.csv\".format(base_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
